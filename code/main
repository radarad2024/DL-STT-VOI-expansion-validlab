"""
3D MRI Classification Training - DataParallel Optimized Version
RTX 4080 x2 + i9-14900K + 64GB RAM Optimization
Train on train set only for each fold and evaluate on test set
"""

# %% Cell 1: Library imports and settings
import os
import sys
import gc
import warnings
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Any
import time
from datetime import datetime
import multiprocessing
import json
import pickle

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, roc_curve, classification_report
from scipy import stats

# tqdm setup
from tqdm import tqdm

# PyTorch / MONAI
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data.dataloader import default_collate
from torch.nn.functional import softmax

import matplotlib.pyplot as plt
import seaborn as sns
import monai
from monai.data import Dataset as MonaiDataset
from monai.transforms import (
    Compose, LoadImaged, EnsureChannelFirstd, Spacingd,
    Resized, ScaleIntensityd, MaskIntensityd,
    NormalizeIntensityd, RandFlipd, RandAffined,
    RandGaussianNoised, RandBiasFieldd, Rand3DElasticd,
    ToTensord
)
from monai.networks.nets import DenseNet121
from monai.utils import set_determinism

warnings.filterwarnings("ignore")

# CUDNN optimization settings
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False  # Speed priority

def collate_remove_none(batch):
    """Filter out None samples from SafeDataset batching"""
    batch = [b for b in batch if b is not None]
    if not batch:
        return None
    return default_collate(batch)

class SafeDataset(MonaiDataset):
    """Wrap MONAI Dataset to skip items that fail during transforms"""
    def __getitem__(self, index):
        try:
            return super().__getitem__(index)
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping sample {index} due to error: {e}")
            return None

# GPU setup verification
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    # VRAM information output
    props = torch.cuda.get_device_properties(i)
    print(f"  - Total memory: {props.total_memory / 1024**3:.2f} GB")
print(f"CPU cores: {multiprocessing.cpu_count()}")

# %% Cell 2: Configuration class definition (DataParallel optimized)
@dataclass
class TrainingConfig:
    # Basic paths and data settings
    root_dir: str = " " # Root directory 
    label_file: str = "labels.csv"
    
    # Model settings - GPU allocation removed (using DataParallel)
    model_configs: Dict[str, Dict] = field(default_factory=lambda: {
        "Model_1_BoundingBox": {
            "dataset_type": "normalization_BoundingBox", 
            "description": "Tight Bounding Box Model"
        },
        "Model_2_StandardVOI": {
            "dataset_type": "normalization_ROI", 
            "description": "Standard VOI Model"
        },
        "Model_3_ExpandedVOI": {
            "dataset_type": "normalization_Dilation1cm", 
            "description": "Expanded VOI Model (+1cm)"
        }
    })
    
    # Training settings (RTX 4080 x2 optimized)
    n_folds: int = 5
    epochs: int = 30  # Fixed training epochs (no early stopping)
    final_epochs: int = 30  # Epochs for final model training
    batch_size: int = 32  # DataParallel: 16 per GPU
    learning_rate: float = 1e-4  # Adjusted for increased batch
    use_cuda: bool = True
    num_workers: int = 16  # Utilizing i9-14900K 24 cores
    pin_memory: bool = True
    persistent_workers: bool = True  # Reduce worker recreation overhead
    prefetch_factor: int = 4  # Increase data prefetch
    
    # DataParallel settings
    use_data_parallel: bool = True
    data_parallel_device_ids: List[int] = field(default_factory=lambda: [0, 1])
    
    # Image preprocessing settings
    target_spacing: Tuple[float,float,float] = (1.0, 1.0, 2.0)
    roi_size: Tuple[int,int,int] = (128, 128, 80)
    
    # Random seed
    seed: int = 42
    
    # Save settings
    save_results: bool = True
    output_dir: str = "output_dataparallel_optimized"
    save_checkpoints: bool = True
    checkpoint_interval: int = 10  # Checkpoint save interval
    
    # Statistics settings
    bootstrap_n_samples: int = 2000
    confidence_level: float = 0.95
    
    # Data Augmentation probability settings
    aug_flip_prob: float = 0.5
    aug_affine_prob: float = 0.8
    aug_noise_prob: float = 0.3
    aug_bias_prob: float = 0.2
    aug_elastic_prob: float = 0.1
    
    # Performance optimization settings
    use_mixed_precision: bool = True
    use_tensorboard: bool = False
    use_class_weights: bool = True
    gradient_clip_val: float = 1.0
    gradient_accumulation_steps: int = 1  # Batch size sufficient
    
    # Memory cleanup interval
    memory_cleanup_interval: int = 25  # More frequent memory cleanup
    
    # Time tracking
    track_time: bool = True
    time_limit_hours: float = 48.0  # 48 hour limit

# %% Cell 3: Time tracking utility
class TimeTracker:
    """Experiment time tracking and prediction"""
    def __init__(self, total_epochs: int, total_folds: int, total_models: int, time_limit_hours: float = 48.0):
        self.start_time = time.time()
        self.total_epochs = total_epochs
        self.total_folds = total_folds
        self.total_models = total_models
        self.time_limit_hours = time_limit_hours
        
        self.epoch_times = []
        self.fold_times = []
        self.model_times = []
        
        self.current_model = None
        self.current_fold = None
        self.epoch_start = None
        self.fold_start = None
        self.model_start = None
    
    def start_model(self, model_name: str):
        self.current_model = model_name
        self.model_start = time.time()
        print(f"\n‚è±Ô∏è Starting {model_name} at {datetime.now().strftime('%H:%M:%S')}")
    
    def end_model(self):
        if self.model_start:
            model_time = time.time() - self.model_start
            self.model_times.append(model_time)
            print(f"‚è±Ô∏è Model completed in {model_time/3600:.2f} hours")
            self.estimate_remaining_time()
    
    def start_fold(self, fold: int):
        self.current_fold = fold
        self.fold_start = time.time()
    
    def end_fold(self):
        if self.fold_start:
            fold_time = time.time() - self.fold_start
            self.fold_times.append(fold_time)
            print(f"   Fold completed in {fold_time/60:.1f} minutes")
    
    def start_epoch(self):
        self.epoch_start = time.time()
    
    def end_epoch(self):
        if self.epoch_start:
            epoch_time = time.time() - self.epoch_start
            self.epoch_times.append(epoch_time)
            return epoch_time
        return 0
    
    def estimate_remaining_time(self):
        """Estimate remaining time"""
        elapsed = time.time() - self.start_time
        
        if self.epoch_times:
            avg_epoch_time = np.mean(self.epoch_times[-20:])  # Average of last 20
            completed_epochs = len(self.epoch_times)
            total_epochs_all = self.total_epochs * self.total_folds * self.total_models
            remaining_epochs = total_epochs_all - completed_epochs
            
            estimated_remaining = remaining_epochs * avg_epoch_time
            estimated_total = elapsed + estimated_remaining
            
            print(f"\nüìä Time Statistics:")
            print(f"   Average epoch time: {avg_epoch_time:.1f}s")
            print(f"   Elapsed time: {elapsed/3600:.2f} hours")
            print(f"   Estimated remaining: {estimated_remaining/3600:.2f} hours")
            print(f"   Estimated total: {estimated_total/3600:.2f} hours")
            print(f"   Progress: {completed_epochs}/{total_epochs_all} epochs ({100*completed_epochs/total_epochs_all:.1f}%)")
            
            if estimated_total/3600 > self.time_limit_hours:
                print(f"   ‚ö†Ô∏è WARNING: Estimated to exceed {self.time_limit_hours}h limit!")
            else:
                print(f"   ‚úÖ On track to complete within {self.time_limit_hours}h limit")
            
            return estimated_remaining
        return None

# %% Cell 4: Statistics utility functions
class StatisticsUtils:
    @staticmethod
    def bootstrap_auc(y_true: np.ndarray, y_prob: np.ndarray, n_bootstrap: int = 2000, 
                     confidence_level: float = 0.95, random_state: int = 42) -> Dict[str, float]:
        """Calculate AUC 95% confidence interval using Bootstrap"""
        np.random.seed(random_state)
        n_samples = len(y_true)
        bootstrap_aucs = []
        
        for _ in range(n_bootstrap):
            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)
            bootstrap_y_true = y_true[bootstrap_indices]
            bootstrap_y_prob = y_prob[bootstrap_indices]
            
            if len(np.unique(bootstrap_y_true)) > 1:
                bootstrap_auc = roc_auc_score(bootstrap_y_true, bootstrap_y_prob)
                bootstrap_aucs.append(bootstrap_auc)
        
        bootstrap_aucs = np.array(bootstrap_aucs)
        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        return {
            'auc_mean': np.mean(bootstrap_aucs),
            'auc_std': np.std(bootstrap_aucs),
            'auc_ci_lower': np.percentile(bootstrap_aucs, lower_percentile),
            'auc_ci_upper': np.percentile(bootstrap_aucs, upper_percentile),
            'bootstrap_samples': len(bootstrap_aucs)
        }
    
    @staticmethod
    def find_optimal_cutoff_youden(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:
        """Find optimal cutoff using Youden Index"""
        fpr, tpr, thresholds = roc_curve(y_true, y_prob)
        youden_index = tpr - fpr
        optimal_idx = np.argmax(youden_index)
        
        optimal_cutoff = thresholds[optimal_idx]
        y_pred = (y_prob >= optimal_cutoff).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        return {
            'cutoff_youden': optimal_cutoff,
            'youden_index': youden_index[optimal_idx],
            'sens_youden': sensitivity,
            'spec_youden': specificity
        }
    
    @staticmethod
    def find_optimal_cutoff_f1(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:
        """Find optimal cutoff maximizing F1 Score"""
        thresholds = np.linspace(0.01, 0.99, 99)
        f1_scores = []
        
        for threshold in thresholds:
            y_pred = (y_prob >= threshold).astype(int)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            f1_scores.append(f1)
        
        optimal_idx = np.argmax(f1_scores)
        optimal_cutoff = thresholds[optimal_idx]
        optimal_f1 = f1_scores[optimal_idx]
        
        y_pred = (y_prob >= optimal_cutoff).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        return {
            'cutoff_f1': optimal_cutoff,
            'f1_score': optimal_f1,
            'sens_f1': sensitivity,
            'spec_f1': specificity
        }

# %% Cell 5: Result logger class
class ResultLogger:
    """Class for systematically saving all training results"""
    def __init__(self, output_dir: str, model_name: str, fold: int):
        self.base_dir = os.path.join(output_dir, model_name, f"fold_{fold}")
        os.makedirs(self.base_dir, exist_ok=True)
        
        # Create subdirectories
        self.dirs = {
            'metrics': os.path.join(self.base_dir, 'metrics'),
            'plots': os.path.join(self.base_dir, 'plots'),
            'predictions': os.path.join(self.base_dir, 'predictions'),
            'logs': os.path.join(self.base_dir, 'logs')
        }
        
        for dir_path in self.dirs.values():
            os.makedirs(dir_path, exist_ok=True)
        
        # Initialize log file
        self.log_file = os.path.join(self.dirs['logs'], 'training_log.txt')
        self.epoch_metrics = []
        
        # Record start time
        self.start_time = time.time()
        self.log(f"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    def log(self, message: str):
        """Save log message"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        
        # Console output
        print(log_message)
        
        # Save to file
        with open(self.log_file, 'a') as f:
            f.write(log_message + '\n')
    
    def save_epoch_metrics(self, epoch: int, train_loss: float, epoch_time: float = None):
        """Save per-epoch metrics"""
        metrics = {
            'epoch': epoch,
            'train_loss': train_loss,
            'epoch_time': epoch_time,
            'timestamp': datetime.now().isoformat()
        }
        
        self.epoch_metrics.append(metrics)
        
        # Save as JSON
        json_path = os.path.join(self.dirs['metrics'], 'epoch_metrics.json')
        with open(json_path, 'w') as f:
            json.dump(self.epoch_metrics, f, indent=2)
    
    def save_test_predictions(self, y_true: List[int], y_prob: List[float]):
        """Save test prediction results"""
        df = pd.DataFrame({
            'y_true': y_true,
            'y_prob': y_prob,
            'y_pred_0.5': (np.array(y_prob) >= 0.5).astype(int)
        })
        
        csv_path = os.path.join(self.dirs['predictions'], 'test_predictions.csv')
        df.to_csv(csv_path, index=False)
        
        # Visualize distribution
        self._plot_prediction_distribution(y_true, y_prob)
    
    def _plot_prediction_distribution(self, y_true: List[int], y_prob: List[float]):
        """Visualize prediction probability distribution"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram
        y_true_arr = np.array(y_true)
        y_prob_arr = np.array(y_prob)
        
        ax1.hist(y_prob_arr[y_true_arr == 0], bins=30, alpha=0.7, label='Class 0', color='blue')
        ax1.hist(y_prob_arr[y_true_arr == 1], bins=30, alpha=0.7, label='Class 1', color='red')
        ax1.set_xlabel('Predicted Probability')
        ax1.set_ylabel('Count')
        ax1.set_title('Test Set - Prediction Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true_arr, y_prob_arr)
        auc = roc_auc_score(y_true_arr, y_prob_arr)
        
        ax2.plot(fpr, tpr, label=f'ROC (AUC = {auc:.3f})')
        ax2.plot([0, 1], [0, 1], 'k--', label='Random')
        ax2.set_xlabel('False Positive Rate')
        ax2.set_ylabel('True Positive Rate')
        ax2.set_title('Test Set - ROC Curve')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(self.dirs['plots'], 'test_prediction_analysis.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_confusion_matrix(self, y_true: List[int], y_pred: List[int]):
        """Save confusion matrix"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['Predicted 0', 'Predicted 1'],
                    yticklabels=['True 0', 'True 1'])
        plt.title('Test Set - Confusion Matrix')
        plt.tight_layout()
        
        plot_path = os.path.join(self.dirs['plots'], 'test_confusion_matrix.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Also save as text
        report = classification_report(y_true, y_pred, output_dict=True)
        report_path = os.path.join(self.dirs['metrics'], 'test_classification_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
    
    def save_final_results(self, results: Dict):
        """Save final results"""
        # Save as JSON format
        json_path = os.path.join(self.dirs['metrics'], 'final_results.json')
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Generate summary text
        summary_path = os.path.join(self.base_dir, 'summary.txt')
        with open(summary_path, 'w') as f:
            f.write(f"Training Summary\n")
            f.write(f"="*50 + "\n")
            f.write(f"Total training time: {(time.time() - self.start_time) / 60:.2f} minutes\n")
            f.write(f"Total epochs: {results.get('total_epochs', 'N/A')}\n")
            f.write(f"Final train loss: {results.get('final_train_loss', 0):.4f}\n")
            f.write(f"Test AUC: {results.get('test_auc', 0):.4f}\n")
            f.write(f"\nTest Metrics:\n")
            
            if 'test_metrics' in results and results['test_metrics']:
                for key, value in results['test_metrics'].items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
    
    def plot_learning_curves(self, train_losses: List[float], epoch_times: List[float] = None):
        """Plot learning curves"""
        epochs = list(range(1, len(train_losses) + 1))
        
        fig, axes = plt.subplots(1, 2 if epoch_times else 1, figsize=(15 if epoch_times else 10, 6))
        
        if epoch_times:
            ax1, ax2 = axes
        else:
            ax1 = axes
        
        # Loss curve
        ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Time per epoch
        if epoch_times:
            ax2.plot(epochs, epoch_times, 'g-', label='Epoch Time', linewidth=2)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Time (seconds)')
            ax2.set_title('Training Time per Epoch')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(self.dirs['plots'], 'learning_curves.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Also save as CSV
        df = pd.DataFrame({
            'epoch': epochs,
            'train_loss': train_losses
        })
        if epoch_times:
            df['epoch_time'] = epoch_times
        
        csv_path = os.path.join(self.dirs['metrics'], 'learning_curves.csv')
        df.to_csv(csv_path, index=False)

# %% Cell 6: Data helper functions
def prepare_data_dict(item: Dict, labels: Dict) -> Dict:
    """Prepare data dictionary"""
    return {
        "en_img": item["en"], "t1_img": item["t1"], "t2_img": item["t2"],
        "en_lbl": item["seg_en"], "t1_lbl": item["seg_en"], "t2_lbl": item["seg_en"],
        "label": labels[item["patient_id"]], "patient_id": item["patient_id"],
    }

# %% Cell 7: Transform definitions
class TransformManager:
    def __init__(self, config: TrainingConfig):
        self.config = config
    
    def get_transforms(self, is_training: bool = True) -> Compose:
        keys = ["en_img", "t1_img", "t2_img", "en_lbl", "t1_lbl", "t2_lbl"]
        transforms = [
            LoadImaged(keys=keys, image_only=True),
            EnsureChannelFirstd(keys=keys),
            Spacingd(keys=keys, pixdim=self.config.target_spacing, mode=["bilinear"]*3+["nearest"]*3),
            Resized(keys=keys, spatial_size=self.config.roi_size, mode=["trilinear"]*3+["nearest"]*3),
            ScaleIntensityd(keys=["en_img", "t1_img", "t2_img"], minv=0.0, maxv=1.0),
            MaskIntensityd(keys=["en_img", "t1_img", "t2_img"], mask_key="en_lbl"),
            NormalizeIntensityd(keys=["en_img", "t1_img", "t2_img"], nonzero=True)
        ]
        
        if is_training:
            transforms += [
                RandFlipd(keys=keys, prob=self.config.aug_flip_prob, spatial_axis=0),
                RandFlipd(keys=keys, prob=self.config.aug_flip_prob, spatial_axis=1),
                RandAffined(keys=keys, mode=["bilinear"]*3+["nearest"]*3,
                           prob=self.config.aug_affine_prob, rotate_range=[0.1]*3,
                           translate_range=[10, 10, 5], scale_range=[0.1]*3),
                RandGaussianNoised(keys=["en_img", "t1_img", "t2_img"], 
                                  prob=self.config.aug_noise_prob, mean=0.0, std=0.1),
                RandBiasFieldd(keys=["en_img", "t1_img", "t2_img"], 
                              prob=self.config.aug_bias_prob, degree=3, coeff_range=(0.0, 0.1)),
                Rand3DElasticd(keys=keys, prob=self.config.aug_elastic_prob, 
                            sigma_range=(5, 8), magnitude_range=(100, 200),
                            spatial_size=self.config.roi_size, 
                            mode=["bilinear"]*3+["nearest"]*3)
            ]
        
        transforms.append(ToTensord(keys=["en_img", "t1_img", "t2_img"]))
        return Compose(transforms)

# %% Cell 8: Data management
class DataManager:
    def __init__(self, config: TrainingConfig):
        self.config = config
    
    def load_labels(self) -> Dict[str, int]:
        path = os.path.join(self.config.root_dir, self.config.label_file)
        if not os.path.exists(path):
            raise FileNotFoundError(f"Label file not found: {path}")
        
        df = pd.read_csv(path)
        label_dict = df.set_index("patient_id")["label"].astype(int).to_dict()
        print(f"‚úÖ Loaded {len(label_dict)} labels")
        
        # Output class distribution
        labels_array = np.array(list(label_dict.values()))
        unique, counts = np.unique(labels_array, return_counts=True)
        print(f"   Class distribution: {dict(zip(unique, counts))}")
        
        return label_dict
    
    def build_file_list(self, dataset_type: str, labels: Dict[str, int]) -> List[Dict[str, str]]:
        base = os.path.join(self.config.root_dir, dataset_type)
        if not os.path.exists(base):
            raise FileNotFoundError(f"Dataset directory not found: {base}")
        
        seq_dirs = {k: None for k in ["en", "t1", "t2"]}
        for d in os.listdir(base):
            ld = d.lower()
            for k in seq_dirs:
                if seq_dirs[k] is None and k in ld:
                    seq_dirs[k] = os.path.join(base, d)
        
        items = []
        for pid in os.listdir(seq_dirs["en"]):
            if pid not in labels:
                continue
                
            def find_files(seq_dir):
                p = os.path.join(seq_dir, pid)
                if not os.path.isdir(p):
                    return None, None
                fs = [f for f in os.listdir(p) if f.lower().endswith(".nii.gz")]
                img, seg = None, None
                for f in fs:
                    if f.lower().startswith("seg"):
                        seg = os.path.join(p, f)
                    else:
                        img = os.path.join(p, f)
                return img, seg
            
            en, es = find_files(seq_dirs["en"])
            t1, _ = find_files(seq_dirs["t1"])
            t2, _ = find_files(seq_dirs["t2"])
            
            if all([en, t1, t2, es]):
                items.append({
                    "patient_id": pid, "en": en, "t1": t1, "t2": t2, "seg_en": es
                })
        
        print(f"‚úÖ Found {len(items)} patient datasets")
        return items

# %% Cell 9: Training engine (DataParallel optimized)
class TrainingEngine:
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device("cuda:0" if config.use_cuda and torch.cuda.is_available() else "cpu")
        set_determinism(seed=config.seed)
        np.random.seed(config.seed)
        torch.manual_seed(config.seed)
        
        # Mixed precision settings (more aggressive scaling)
        self.scaler = torch.cuda.amp.GradScaler(
            init_scale=2.**16,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=2000,
            enabled=config.use_mixed_precision
        ) if config.use_mixed_precision else None
        
        if config.save_results:
            os.makedirs(config.output_dir, exist_ok=True)

    def create_model(self) -> nn.Module:
        # Create MONAI DenseNet121 (for 3D medical imaging)
        model = DenseNet121(
            spatial_dims=3, 
            in_channels=3, 
            out_channels=2
        )
        
        # Apply DataParallel
        if self.config.use_data_parallel and torch.cuda.device_count() > 1:
            print(f"‚úÖ Using DataParallel on GPUs: {self.config.data_parallel_device_ids}")
            model = nn.DataParallel(model, device_ids=self.config.data_parallel_device_ids)
        
        model = model.to(self.device)
        print(f"‚úÖ Model loaded on {self.device}")
        print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
        
        return model
    
    def get_class_weights(self, labels: List[int]) -> torch.Tensor:
        """Calculate class weights"""
        class_counts = np.bincount(labels)
        class_weights = len(labels) / (len(class_counts) * class_counts)
        return torch.FloatTensor(class_weights).to(self.device)
    
    def cleanup_gpu_memory(self):
        """Clean up GPU memory"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

    def train_epoch(self, model: nn.Module, train_loader: DataLoader,
                   optimizer: torch.optim.Optimizer, criterion: nn.Module,
                   epoch: int = None, logger: Optional[ResultLogger] = None,
                   time_tracker: Optional[TimeTracker] = None) -> Tuple[float, float]:
        """Train epoch - with time tracking"""
        model.train()
        total_loss = 0.0
        count = 0
        
        if time_tracker:
            time_tracker.start_epoch()
        
        batch_times = []
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch} [train]", unit="batch")):
            if batch is None:
                continue
            
            batch_start = time.time()
            
            try:
                imgs = torch.cat([batch["en_img"], batch["t1_img"], batch["t2_img"]], dim=1).to(self.device)
                labels = batch["label"].to(self.device)
                
                # Gradient accumulation support
                if batch_idx % self.config.gradient_accumulation_steps == 0:
                    optimizer.zero_grad()
                
                # Mixed precision training
                if self.config.use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = model(imgs)
                        loss = criterion(outputs, labels)
                        loss = loss / self.config.gradient_accumulation_steps
                    
                    self.scaler.scale(loss).backward()
                    
                    if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                        if self.config.gradient_clip_val > 0:
                            self.scaler.unscale_(optimizer)
                            torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.gradient_clip_val)
                        
                        self.scaler.step(optimizer)
                        self.scaler.update()
                else:
                    outputs = model(imgs)
                    loss = criterion(outputs, labels)
                    loss = loss / self.config.gradient_accumulation_steps
                    loss.backward()
                    
                    if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                        if self.config.gradient_clip_val > 0:
                            torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.gradient_clip_val)
                        
                        optimizer.step()
                
                bs = imgs.size(0)
                total_loss += loss.item() * bs * self.config.gradient_accumulation_steps
                count += bs
                
                batch_times.append(time.time() - batch_start)
                
                # Periodic memory cleanup
                if batch_idx % self.config.memory_cleanup_interval == 0:
                    self.cleanup_gpu_memory()
                    
            except RuntimeError as e:
                if "out of memory" in str(e):
                    if logger:
                        logger.log(f"‚ö†Ô∏è GPU OOM detected! Skipping batch...")
                    self.cleanup_gpu_memory()
                    continue
                else:
                    raise e
        
        epoch_time = 0
        if time_tracker:
            epoch_time = time_tracker.end_epoch()
        
        avg_batch_time = np.mean(batch_times) if batch_times else 0
        
        return total_loss / count if count > 0 else 0.0, epoch_time

    def test_model(self, model: nn.Module, test_loader: DataLoader,
                  logger: Optional[ResultLogger] = None) -> Tuple[Dict[str, float], Dict[str, List[float]]]:
        """Evaluate model on test set"""
        model.eval()
        y_true, y_prob = [], []
        
        test_loader_desc = "Testing"
        if logger:
            logger.log("Starting test evaluation...")
        
        for batch in tqdm(test_loader, desc=test_loader_desc, unit="batch"):
            if batch is None:
                continue
                
            imgs = torch.cat([batch["en_img"], batch["t1_img"], batch["t2_img"]], dim=1).to(self.device)
            labels = batch["label"].to(self.device)
            
            with torch.no_grad():
                if self.config.use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = model(imgs)
                else:
                    outputs = model(imgs)
                
                probs = softmax(outputs, dim=1)[:, 1]
            
            y_true.extend(labels.cpu().tolist())
            y_prob.extend(probs.cpu().tolist())
        
        metrics = self.compute_metrics(y_true, y_prob)
        
        return metrics, {"y_true": y_true, "y_prob": y_prob}

    def compute_metrics(self, y_true: List[int], y_prob: List[float]) -> Dict[str, Any]:
        """Compute complete metrics"""
        y_true_arr = np.array(y_true)
        y_prob_arr = np.array(y_prob)
        
        try:
            auc = roc_auc_score(y_true_arr, y_prob_arr)
        except:
            auc = 0.0
        
        bootstrap_results = StatisticsUtils.bootstrap_auc(
            y_true_arr, y_prob_arr, 
            n_bootstrap=self.config.bootstrap_n_samples,
            confidence_level=self.config.confidence_level
        )
        
        youden_results = StatisticsUtils.find_optimal_cutoff_youden(y_true_arr, y_prob_arr)
        f1_results = StatisticsUtils.find_optimal_cutoff_f1(y_true_arr, y_prob_arr)
        
        metrics = {
            'auc': auc,
            **bootstrap_results,
            **youden_results,
            **f1_results
        }
        
        return metrics

# %% Cell 10: Model training manager
class ModelTrainer:
    def __init__(self, config: TrainingConfig, time_tracker: Optional[TimeTracker] = None):
        self.config = config
        self.data_manager = DataManager(config)
        self.transform_manager = TransformManager(config)
        self.training_engine = TrainingEngine(config)
        self.time_tracker = time_tracker

    def train_single_fold(self, model_name: str, fold: int, train_items, test_items, labels):
        print(f"\nüìÅ Fold {fold}/{self.config.n_folds}")
        
        if self.time_tracker:
            self.time_tracker.start_fold(fold)
        
        # Initialize logger
        logger = ResultLogger(self.config.output_dir, model_name, fold)
        logger.log(f"Starting fold {fold} training for {model_name}")
        logger.log(f"Train samples: {len(train_items)}, Test samples: {len(test_items)}")
        logger.log(f"Learning rate: {self.config.learning_rate}")
        logger.log(f"Batch size: {self.config.batch_size} (effective per GPU: {self.config.batch_size // torch.cuda.device_count()})")
        logger.log(f"Epochs: {self.config.epochs} (no early stopping)")
        
        # Prepare transforms
        train_trans = self.transform_manager.get_transforms(True)
        test_trans = self.transform_manager.get_transforms(False)
        
        # Prepare data
        train_data = [prepare_data_dict(i, labels) for i in train_items]
        test_data = [prepare_data_dict(i, labels) for i in test_items]
        
        train_ds = SafeDataset(data=train_data, transform=train_trans)
        test_ds = SafeDataset(data=test_data, transform=test_trans)
        
        # Data loaders
        train_loader = DataLoader(
            train_ds, batch_size=self.config.batch_size, shuffle=True,
            num_workers=self.config.num_workers, pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers, 
            prefetch_factor=self.config.prefetch_factor,
            collate_fn=collate_remove_none,
            drop_last=True
        )
        test_loader = DataLoader(
            test_ds, batch_size=16, shuffle=False,  # Smaller batch size for test
            num_workers=8, pin_memory=self.config.pin_memory,
            persistent_workers=False, collate_fn=collate_remove_none
        )

        # Model and optimizer setup
        model = self.training_engine.create_model()
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.learning_rate, weight_decay=1e-5)
        
        # Scheduler setup (using CosineAnnealingWarmRestarts)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2, eta_min=1e-6
        )
        
        # Calculate class weights
        train_labels = [labels[item['patient_id']] for item in train_items]
        if self.config.use_class_weights:
            class_weights = self.training_engine.get_class_weights(train_labels)
            criterion = nn.CrossEntropyLoss(weight=class_weights)
            logger.log(f"Using class weights: {class_weights.cpu().numpy()}")
        else:
            criterion = nn.CrossEntropyLoss()

        # Training loop
        train_losses = []
        epoch_times = []
        
        for epoch in range(1, self.config.epochs + 1):
            logger.log(f"\nüìä Epoch {epoch}/{self.config.epochs}")
            
            # Train
            train_loss, epoch_time = self.training_engine.train_epoch(
                model, train_loader, optimizer, criterion, 
                epoch=epoch, logger=logger, time_tracker=self.time_tracker
            )
            train_losses.append(train_loss)
            epoch_times.append(epoch_time)
            scheduler.step()
            
            current_lr = optimizer.param_groups[0]['lr']
            logger.log(f"   Train Loss: {train_loss:.4f}, LR: {current_lr:.2e}, Time: {epoch_time:.1f}s")
            
            # Save epoch metrics
            logger.save_epoch_metrics(epoch, train_loss, epoch_time)
            
            # Save checkpoint
            if self.config.save_checkpoints and epoch % self.config.checkpoint_interval == 0:
                ckpt_dir = os.path.join(self.config.output_dir, model_name, f"fold_{fold}", "checkpoints")
                os.makedirs(ckpt_dir, exist_ok=True)
                ckpt_path = os.path.join(ckpt_dir, f"epoch_{epoch}.pth")
                
                # Handle DataParallel
                model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
                
                checkpoint = {
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss
                }
                torch.save(checkpoint, ckpt_path)
                logger.log(f"   üíæ Checkpoint saved at epoch {epoch}")
            
            # GPU memory monitoring
            if epoch % 10 == 0:
                for i in range(torch.cuda.device_count()):
                    memory_used = torch.cuda.memory_allocated(i) / 1024**3
                    memory_cached = torch.cuda.memory_reserved(i) / 1024**3
                    logger.log(f"   GPU {i} Memory: {memory_used:.2f}GB used, {memory_cached:.2f}GB cached")

        # Test evaluation after training completion
        logger.log(f"\nüîç Evaluating on test set...")
        test_metrics, test_preds = self.training_engine.test_model(model, test_loader, logger)
        
        # Save test results
        logger.save_test_predictions(test_preds['y_true'], test_preds['y_prob'])
        
        # Save confusion matrix with optimal cutoff
        y_pred_youden = (np.array(test_preds['y_prob']) >= test_metrics['cutoff_youden']).astype(int)
        logger.save_confusion_matrix(test_preds['y_true'], y_pred_youden.tolist())
        
        # Log test results
        logger.log(f"\nüìä Test Results:")
        logger.log(f"   AUC: {test_metrics['auc']:.4f} (CI: {test_metrics['auc_ci_lower']:.3f}-{test_metrics['auc_ci_upper']:.3f})")
        logger.log(f"   Youden - Sens: {test_metrics['sens_youden']:.3f}, Spec: {test_metrics['spec_youden']:.3f}, Cutoff: {test_metrics['cutoff_youden']:.3f}")
        logger.log(f"   F1 Max - Sens: {test_metrics['sens_f1']:.3f}, Spec: {test_metrics['spec_f1']:.3f}, F1: {test_metrics['f1_score']:.3f}")
        
        # Save learning curves
        logger.plot_learning_curves(train_losses, epoch_times)
        
        # Save final model
        model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
        
        final_model_path = os.path.join(self.config.output_dir, model_name, f"fold_{fold}", "final_model.pth")
        final_checkpoint = {
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': self.config.epochs,
            'final_train_loss': train_losses[-1],
            'test_metrics': test_metrics,
            'training_history': train_losses,
            'epoch_times': epoch_times
        }
        torch.save(final_checkpoint, final_model_path)
        logger.log(f"   üíæ Final model saved")

        # Save final results
        final_results = {
            'fold': fold,
            'total_epochs': self.config.epochs,
            'final_train_loss': train_losses[-1],
            'test_auc': test_metrics['auc'],
            'test_metrics': test_metrics,
            'avg_epoch_time': np.mean(epoch_times),
            'total_fold_time': sum(epoch_times)
        }
        
        logger.save_final_results(final_results)
        
        if self.time_tracker:
            self.time_tracker.end_fold()
        
        # Memory cleanup
        del model, train_loader, test_loader
        gc.collect()
        torch.cuda.empty_cache()
        
        return final_results

    def train_model(self, name: str, cfg: Dict) -> Dict:
        print(f"\nüöÄ Training {name}")
        
        labels = self.data_manager.load_labels()
        items = self.data_manager.build_file_list(cfg['dataset_type'], labels)
        
        pids = [i['patient_id'] for i in items]
        y = [labels[p] for p in pids]
        skf = StratifiedKFold(n_splits=self.config.n_folds, shuffle=True, random_state=self.config.seed)
        
        fold_results = []
        for f, (train_idx, test_idx) in enumerate(skf.split(pids, y), 1):
            train_items = [items[i] for i in train_idx]
            test_items = [items[i] for i in test_idx]
            
            fold_result = self.train_single_fold(name, f, train_items, test_items, labels)
            fold_results.append(fold_result)
        
        aggregated_metrics = self.aggregate_results(fold_results)
        
        # Save overall results for model
        self.save_model_summary(name, fold_results, aggregated_metrics)
        
        return {
            'model_name': name,
            'fold_results': fold_results,
            'aggregated_metrics': aggregated_metrics
        }

    def aggregate_results(self, fold_results: List[Dict]) -> Dict[str, float]:
        """Aggregate fold results"""
        metric_names = [
            'auc', 'auc_mean', 'auc_ci_lower', 'auc_ci_upper',
            'sens_youden', 'spec_youden', 'cutoff_youden', 'youden_index',
            'sens_f1', 'spec_f1', 'cutoff_f1', 'f1_score'
        ]
        
        aggregated = {}
        
        for metric in metric_names:
            values = []
            for fold_result in fold_results:
                test_metrics = fold_result.get('test_metrics')
                if test_metrics and metric in test_metrics:
                    values.append(test_metrics[metric])
            
            if values:
                aggregated[f"mean_{metric}"] = np.mean(values)
                aggregated[f"std_{metric}"] = np.std(values)
                aggregated[f"median_{metric}"] = np.median(values)
                aggregated[f"min_{metric}"] = np.min(values)
                aggregated[f"max_{metric}"] = np.max(values)
        
        # Add time statistics
        if 'avg_epoch_time' in fold_results[0]:
            epoch_times = [r['avg_epoch_time'] for r in fold_results]
            aggregated['mean_epoch_time'] = np.mean(epoch_times)
            
        if 'total_fold_time' in fold_results[0]:
            fold_times = [r['total_fold_time'] for r in fold_results]
            aggregated['total_model_time'] = sum(fold_times)
        
        return aggregated

    def save_model_summary(self, model_name: str, fold_results: List[Dict], 
                          aggregated_metrics: Dict):
        """Save overall model summary"""
        model_dir = os.path.join(self.config.output_dir, model_name)
        os.makedirs(model_dir, exist_ok=True)
        
        # Overall results JSON
        results = {
            'model_name': model_name,
            'fold_results': fold_results,
            'aggregated_metrics': aggregated_metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        json_path = os.path.join(model_dir, 'model_summary.json')
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Summary report
        report_path = os.path.join(model_dir, 'model_report.txt')
        with open(report_path, 'w') as f:
            f.write(f"Model: {model_name}\n")
            f.write("="*60 + "\n\n")
            
            # Fold-wise results
            f.write("Fold-wise Test Results:\n")
            f.write("-"*40 + "\n")
            for fold_result in fold_results:
                fold = fold_result['fold']
                auc = fold_result['test_auc']
                f.write(f"Fold {fold}: Test AUC={auc:.4f}\n")
            
            f.write("\n")
            
            # Aggregated results
            f.write("Aggregated Test Results:\n")
            f.write("-"*40 + "\n")
            
            if 'mean_auc' in aggregated_metrics:
                f.write(f"AUC: {aggregated_metrics['mean_auc']:.4f} ¬± {aggregated_metrics.get('std_auc', 0):.4f}\n")
                f.write(f"     Range: [{aggregated_metrics.get('min_auc', 0):.4f}, {aggregated_metrics.get('max_auc', 0):.4f}]\n")
            
            if 'mean_sens_youden' in aggregated_metrics:
                f.write(f"\nYouden Cutoff Performance:\n")
                f.write(f"  Sensitivity: {aggregated_metrics['mean_sens_youden']:.4f} ¬± {aggregated_metrics.get('std_sens_youden', 0):.4f}\n")
                f.write(f"  Specificity: {aggregated_metrics['mean_spec_youden']:.4f} ¬± {aggregated_metrics.get('std_spec_youden', 0):.4f}\n")
                f.write(f"  Cutoff: {aggregated_metrics['mean_cutoff_youden']:.4f} ¬± {aggregated_metrics.get('std_cutoff_youden', 0):.4f}\n")
            
            if 'mean_f1_score' in aggregated_metrics:
                f.write(f"\nF1-Optimized Performance:\n")
                f.write(f"  F1 Score: {aggregated_metrics['mean_f1_score']:.4f} ¬± {aggregated_metrics.get('std_f1_score', 0):.4f}\n")
                f.write(f"  Sensitivity: {aggregated_metrics['mean_sens_f1']:.4f} ¬± {aggregated_metrics.get('std_sens_f1', 0):.4f}\n")
                f.write(f"  Specificity: {aggregated_metrics['mean_spec_f1']:.4f} ¬± {aggregated_metrics.get('std_spec_f1', 0):.4f}\n")
            
            if 'mean_epoch_time' in aggregated_metrics:
                f.write(f"\nTiming Statistics:\n")
                f.write(f"  Average epoch time: {aggregated_metrics['mean_epoch_time']:.1f} seconds\n")
                f.write(f"  Total model training time: {aggregated_metrics.get('total_model_time', 0)/3600:.2f} hours\n")

# %% Cell 11: Sequential execution function (for Jupyter)
def train_all_models_sequential(config: TrainingConfig) -> Dict:
    """Train models sequentially in Jupyter environment"""
    results = {}
    
    # Initialize time tracker
    time_tracker = TimeTracker(
        total_epochs=config.epochs,
        total_folds=config.n_folds,
        total_models=len(config.model_configs),
        time_limit_hours=config.time_limit_hours
    )
    
    # Create base directories for all models first
    for model_name in config.model_configs.keys():
        model_dir = os.path.join(config.output_dir, model_name)
        os.makedirs(model_dir, exist_ok=True)
        print(f"üìÅ Created directory for {model_name}")
    
    # Train models sequentially
    for model_idx, (model_name, model_config) in enumerate(config.model_configs.items(), 1):
        print(f"\n{'='*80}")
        print(f"üöÄ Training Model {model_idx}/{len(config.model_configs)}: {model_name}")
        print(f"{'='*80}")
        
        time_tracker.start_model(model_name)
        
        try:
            trainer = ModelTrainer(config, time_tracker)
            results[model_name] = trainer.train_model(model_name, model_config)
            
            agg_metrics = results[model_name].get('aggregated_metrics', {})
            mean_auc = agg_metrics.get('mean_auc', 0)
            print(f"\n‚úÖ {model_name} completed: Mean Test AUC {mean_auc:.3f}")
            
            time_tracker.end_model()
            
            # Complete memory cleanup
            torch.cuda.empty_cache()
            gc.collect()
            
            # GPU cooldown
            print("   Cooling down GPUs...")
            time.sleep(10)
            
        except Exception as e:
            print(f"‚ùå Error training {model_name}: {e}")
            results[model_name] = {'error': str(e)}
    
    return results

# %% Cell 12: Comparison and save functions
def save_final_comparison(results: Dict, config: TrainingConfig):
    """Save comparison results of all models"""
    comparison_dir = os.path.join(config.output_dir, "model_comparison")
    os.makedirs(comparison_dir, exist_ok=True)
    
    # Save in CSV format
    summary_data = []
    
    for model_name, model_results in results.items():
        if 'error' in model_results:
            continue
            
        agg = model_results.get('aggregated_metrics', {})
        if not agg:
            continue
            
        summary_data.append({
            'model_name': model_name,
            'mean_test_auc': agg.get('mean_auc', np.nan),
            'std_test_auc': agg.get('std_auc', np.nan),
            'auc_ci_lower': agg.get('mean_auc_ci_lower', np.nan),
            'auc_ci_upper': agg.get('mean_auc_ci_upper', np.nan),
            'mean_sens_youden': agg.get('mean_sens_youden', np.nan),
            'mean_spec_youden': agg.get('mean_spec_youden', np.nan),
            'mean_f1_score': agg.get('mean_f1_score', np.nan),
            'mean_epoch_time': agg.get('mean_epoch_time', np.nan),
            'total_time_hours': agg.get('total_model_time', 0) / 3600
        })
    
    if summary_data:
        df = pd.DataFrame(summary_data)
        csv_path = os.path.join(comparison_dir, 'model_comparison.csv')
        df.to_csv(csv_path, index=False)
        
        # Visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # AUC comparison
        ax = axes[0, 0]
        x = range(len(df))
        ax.bar(x, df['mean_test_auc'], yerr=df['std_test_auc'], capsize=5)
